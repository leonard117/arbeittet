%Discounting
Es gibt ein sehr h\"aufiges Problem bei den M-Gramm-Modellen, die aus den Trainingskorpora trainiert werden. Die Trainingskorpora sind immer endlich, d. h.  manche m\"ogliche Wortfolgen der L\"ange m werden im Training nicht beobachtet. Ihre "`Counts"' sind null und entsprechende M-Gramm-Wahrscheinlichkeiten werden zu null gesch\"atzt. Langer Kontext findet keine Ber\"ucksichtigung. Die Wahrscheinlichkeit f\"ur W\"orter, die im Trainingskorpus nicht nahe beieinander liegen, wird in der Regel untersch\"atzt. Beispiel: In Satz  \glqq $Der$ $\underline{Hund}$  $meines$ $Nachbarn$  $\underline{bellte}!$\grqq \space findet man:\\ $P(bellte|Nachbarn, meines, Hund,Der)$ $>$ $P(bellte|Nachbarn)$\\
Um das Problem zu l\"osen setzt man sog. Gl\"attungsverfahren ein, ungesehenen Wortfolgen eine nicht-verschwindende Wahrscheinlichkeit zuordnet. Diese zu ungesehen Ereignissen zugeordnet Wahrscheinlichkeit wird von h\"aufig gesehenen Ereignissen abgezogen.
