%absolute_discounting
Die Grundidee des Algorithmus ist, dass man ein fest Anzahl von der beachteten Wortfolgen abzieht so dass die Wahrscheinlichkeitmass der h\"aufig angesehenen Ereignissen reduziert werden, den die nicht angesehene Ereignisse zugeordnet werden. In \cite{ars_script} wird die ML-Sch\"atzwert durch Absolutes-Dsicounting wie Gleichung (2.9) definiert.
\begin{equation}
P^{*}(w_{n}|w_{n-m+1}^{n-1})=\begin{cases}
\frac{c(w_{n-m+1}^{n-1},w_{n})-d}{c(w_{n-m+1}^{n-1})}+\alpha (w_{n-m+1}^{n-1})\beta (w_{n}|w_{n-m+1}^{n-1}) & c(w_{n-m+1}^{n-1},w_{n})>0 \\
\alpha (w_{n-m+1}^{n-1})\beta (w_{n}|w_{n-m+1}^{n-1}) & c(w_{n-m+1}^{n-1},w_{n})=0 
\end{cases}
\end{equation}
$d$: Discounting Parameter, die von $c(w_{n},w_{n-m+1}^{n-1})$ unabh\"angig ist und von dem ML-Sch\"atzwert abgezogen wird.\\
$d\approx \frac{c_{1}}{c_{1}+c_{2}}$ \\
$c_{1}$:Anzahl der erstmal gesehenen W\"orter.\\
$c_{2}$:Anzahl der zweimal gesehenen W\"orter.\\
$\alpha (w_{n-m+1}^{n-1})$:Normierungskonstante, damit die Gleichung $\sum_{w_{n}}p^{*}(w_{n}|w_{n-m+1}^{n-1})=1$ immer gilt. Die Normierungskonstante ergibt sich aus folgende Rechnung.

\begin{equation}
1=\sum_{w_{n}}p^{*}(w_{n}|w_{n-m+1}^{n-1})=\sum_{w_{n}:c(w_{n-m+1}^{n-1},w_{n})}\frac{c(w_{n-m+1}^{n-1},w_{n})-d}{c(w_{n-m+1}^{n-1})}+\alpha (w_{n-m+1}^{n-1})\sum_{w_{n}}\beta (w_{n}|w_{n-m+1}^{n-1})
\end{equation}
