%backing_off
Das Discounting l\"ost das Problem der Zero-Wahrscheinlichkeit. Backing-off ist auch eine L\"osung. Wenn der ML-Sch\"atzwert eines M-Gramms aus vorgegebenen Daten noch nicht bekannt ist, wird der durch den Sch\"atzenswert des (M-1)-Gramms repr\"asentiert. Beispielsweise wird das Trigramm mit folgender Gleichung (2.23) bezeichnet.
%2.23
\begin{equation}
\label{equation:backing_off_01}
	\hat{p}(w_{i}|w_{i-2},w_{i-1})=
	\begin{cases}
			p(w_{i}|w_{i-2},w_{i-1}) & c(w_{i-2},w_{i-1},w_{i})>0 \\
			\alpha_{1}p(w_{i}|w_{i-1})& c(w_{i-2},w_{i-1},w_{i})=0 and c(w_{i-2},w_{i-1})>0 \\
			\alpha_{1}p(w_{i}) & sonst 
	\end{cases}
\end{equation}
\\
\\
Zus\"atzlich liefert die Backing-off-Methode eine optimierte L\"osung f\"ur das Zero-Wahrsch-einlichkeits-Problem. In den schon vorgestellten Discountings bekommt man alle ungesehenen M-Gramme aus den Gleichungen (2.15) und (2.19) mit gleicher Verteilung, wenn $c(w_{n},w_{n-m+1}^{n-1})=0$ ist. Man kann den Discounting-Algorithmus mit Backing-off kombinieren. Wie im Discounting muss man zuerst die Wahrscheinlichkeit der beachteten m-Gramme reduzieren, um die Wahrscheinlichkeitsmasse f\"ur die niedrigen M-Gramme zu sparen. Der neue  reduzierte Sch\"atzwert wird $P^{*}(w_{n}|w_{n-m+1}^{n-1})$. Wenn $P(w_{n}|w_{n-m+1}^{n-1})=0$ ist, wird die Backing-off-Phase ausgef\"uhrt.  Die Enddarstellung ist die Gleichung (2.24).  Mit  Hilfe des Gewicht $\alpha(w_{n-m+1}^{n-1})$ wird sicher gestellt, dass die Summe der Wahrscheinlichkeiten $w_{n}$ f\"ur alle M-Gramme den Wert 1 erreicht.

%2.24
\begin{equation}
\label{equation:backing_off_02}
\hat{P}(w_{n}|w_{n-m+1}^{n-1})=P^{*}(w_{n}|w_{n-m+1}^{n-1})+\theta(P(w_{n}|w_{n-m+1}^{n-1}))\alpha(w_{n-m+1}^{n-1})P^{*}(w_{n}|w_{n-m+2}^{n-1})
\end{equation}

$\theta(P(w_{n}|w_{n-m+1}^{n-1}))=\begin{cases} 0 & P(w_{n}|w_{n-m+1}^{n-1})>0 \\ 1 & P(w_{n}|w_{n-m+1}^{n-1})= 0 \end{cases}$:bin\"aren Indikatorfunktion.\\
$\alpha(w_{n-m+1}^{n-1})=\frac{1-\sum_{w_{n}:c(w_{n-m+1}^{n-1})>0}P^{*}(w_{n}|w_{n-m+1}^{n-1})}{\sum_{w_{n}:c(w_{n-m+1}^{n-1})=0}P^{*}(w_{n}|w_{n-m+2}^{n-1})}$: Gewicht f\"ur Backing-off.\\
$P^{*}(w_{n}|w_{n-m+2}^{n-1})$: Wahrscheinlichkeit der gesehenen M-Gramme nach Discounting.\\
In der Praxis behandelt man die einmaligen Ereignisse wie die ungesehenen . Nach der Kombination wird aus dem in der Gleichung (2.23) gezeigten Trigramm ein neues wie in der Gleichung (2.25).

%2.25
\begin{equation}
\label{equation:backing_off_03}
\hat{P}(w_{i}|w_{i-2},w_{i-1})=
	\begin{cases}
		P^{*}(w_{i}|w_{i-2},w_{i-1}) & c(w_{i-2},w_{i-1},w_{i})>0\\
		\alpha(w_{n-2}^{n-1})P^{*}(w_{i}|w_{i-1}) & c(w_{i-2},w_{i-1},w_{i})=0 \, and\, c(w_{i-2},w_{i-1})>0\\
		\alpha(w_{n-1})P^{*}(w_{i}) & sonst
	\end{cases}
\end{equation}