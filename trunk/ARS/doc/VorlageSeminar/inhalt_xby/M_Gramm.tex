%M_Gramm
Eine ideale Darstellung des statistischen Sprachmodells ist wie die Gleichung (2.1), die die Wahrscheinlichkeit einer Wortkette bezeichnet. Um die Wahrscheinlichkeit $p(w_{1}^n)$ zu bestimmt m\"ussen die auftretende Wahrscheinlichkeit des Wortes $w_{1}$ und die bedingte Wahrscheinlichkeit f\"ur $w_{2}...w_{n}$ bekannt sein.
\\
\\
\begin{align}
p(w_{1}^n) = p(w_{1},w_{2}...w_{n}) &= \prod_{i=1}^n p(w_{i}|w_{1},..w_{i-1}) \nonumber\\
&= p(w_{1})p(w_{2}|w_{1})...p(w_{n}|w_{1},..w_{n-1})
\end{align}
\\
\\
\cite{book_speech} erkl\"art , dass die Wortkettenl\"ange n theoretisch unendlich gro\ss  sein kann und die Wahrscheinlichkeit $p(w_{n}|w_{1}^n)$ nicht berechenbar wird. Man geht auf eine Approximation der bedingten Wahrscheinlichkeit ein durch Beschr\"ankung der "`History"' in der Bedingung. Die genaue Idee ist, dass man alle bedingten Wahrscheinlichkeiten $p(w_{n}|w_{1}...w_{n})$, f\"ur die die letzten m-1 Vorg\"angerw\"orter von $w_{n}$ identisch sind, zu einer \"Aquivalenzklasse zusammenfasst. Das Rechnen $p(w_{n}|w_{1}...w_{n})$ im Trigramm wird aus $p(w_{n-2})$, $p(w_{n-1}|w_{n-2})$ und $p(w_{n}|w_{n-2},w_{n-1})$ anstatt von allen bedingten Wahrscheinlichkeiten bestimmt. 
Die Gleichung (2.2) ist die mathematische Darstellung f\"ur das M-Gramm, (2.3) f\"ur das Unigramm(m=1), (2.4) f\"ur das Bigramm(m=2) und (2.5) f\"ur das Trigramm(m=3).

\begin{align}
p(w_{1}^n) & \approx \prod_{i=1}^n p(w_{i}|w_{i-m+1}^{i-1})\nonumber \\
					 &=p(w_{1})p(w_{2}|w_{1})p(w_{3}|w_{1},w_{2})...p(w_{n}|w_{n-m+1},...w_{n-1})\\
p(w_{1}^n) & \approx \prod_{i=1}^n p(w_{i})\nonumber \\
					 &=p(w_{1})p(w_{2})p(w_{3})...p(w_{n})\\	
p(w_{1}^n) & \approx \prod_{i=1}^n p(w_{i}|w_{i-1})\nonumber \\
					 &=p(w_{1})p(w_{2}|w_{1})p(w_{3}|w_{2})...p(w_{n}|w_{n-1}) \\				 
p(w_{1}^n) & \approx \prod_{i=1}^n p(w_{i}|w_{i-2},w_{i-1})\nonumber \\
					 &=p(w_{1})p(w_{2}|w_{1})p(w_{3}|w_{1},w_{2})...p(w_{n}|w_{n-2},w_{n-1})
\end{align}
\\
\\
Vorliegende bedingte Wahrscheinlichkeiten k\"onnen mathematisch wie folgende Gleichung (2.6) und (2.7)  aus unbedingten Wahrscheinlichkeiten definiert werden.
%2.6-2.7
\begin{align}
p(w|v) &=\frac{p(v,w)}{p(v)} \\
p(w_{n}|w_{n-m+1}^{n-1}) &=\frac{p(w_{n},w_{n-m+1}^{n-1} )}{p(w_{n-m+1}^{n-1})} 
\end{align}

\begin{itemize}
	\item $p(w_{n},w_{n-m+1}^{n-1})$: Auftrittswahrscheinlichkeit der Wortfolge $w_{n},w_{n-m+1}^{n-1}$.\\
	\item $p(w_{n-m+1}^{n-1})$: Auftrittswahrscheinlichkeit der Wortfolge $w_{n-m+1}^{n-1}$.
\end{itemize}

Aber in der Praxis sind die tats\"achlichen Wahrscheinlichkeiten $p(w_{n-m+1}^{n-1})$  und \\ $p(w_{n},w_{n-m+1}^{n-1})$ nicht bekannt und werden nicht zu dem Rechnen der bedingten Wahrscheinlichkeit $p(w_{n}|w_{n-m+1}^{n-1})$ verwendet. Man trainiert die M-Gramm-Modelle h\"aufig nach der Maximum-Likelihood-Methode \cite{int_MLE} durch Z\"ahlen und Normalisierung wie die Gleichung (2.8). 
\begin{equation}
\hat{p}(w_{n}|w_{n-m+1}^{n-1})=\frac{c(w_{n-m+1}^{n-1},w_{n} )}{ \sum_{i=1}^L c(w_{n-m+1}^{n-1},w_{i})}=\frac{c(w_{n-m+1}^{n-1},w_{n} )}{c(w_{n-m+1}^{n-1})}
\end{equation}
\\
\\
\begin{itemize}
	\item $c(w_{n-m+1}^{n-1},w_{n})$: absolute H\"aufigkeit der Wortfolge $(w_{n-m+1}^{n-1},w_{n})$ in Trainingskorpus\\
	\item $c(w_{n-m+1}^{n-1})$:absolute H\"aufigkeit der Wortfolge $w_{n-m+1}^{n-1}$ in Trainingskorpus\\
\end{itemize}
Hier kann $\hat{p}(w_{n}|w_{n-m+1}^{n-1})$  eine Approximation von der bedingten Wahrscheinlichkeit $p(w_{n}|w_{n-m+1}^{n-1})$ sein und dies wird auch als ML(Maximum-Likelihood)-Sch\"atzwert bezeichnet.\\

%\cite{ars_script}
%\cite{int_MLE}
%\cite{folie_script}
%\cite{int_entropie}
