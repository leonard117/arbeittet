fp = ..//data/target//output/ 
fn = output 
fe = text 
Reading in a 2-gram language model.
Number of 1-grams = 12977.
Number of 2-grams = 81766.
Reading unigrams...

Reading 2-grams...
evallm : Computing perplexity of the language model with respect
   to the text ..//data/target//output//output.text
Perplexity = 73.65, Entropy = 6.20 bits
Computation based on 154998 words.
Number of 2-grams hit = 154998  (100.00%)
Number of 1-grams hit = 0  (0.00%)
0 OOVs (0.00%) and 13592 context cues were removed from the calculation.
evallm : 